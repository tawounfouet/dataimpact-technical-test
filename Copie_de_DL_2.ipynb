{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawounfouet/dataimpact-technical-test/blob/main/Copie_de_DL_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti9sGhemyre4"
      },
      "source": [
        "# Data Science\n",
        "\n",
        "**Answering questions with a ðŸ”´ are considered mandatory for CDI.**\n",
        "\n",
        "Before starting the exercises make sure to enable the GPU\n",
        "\n",
        "\n",
        "*   Go to the runtime menu (&#8593; )\n",
        "*   Select *Change runtime type*\n",
        "*   Select GPU as *Hardware accelerator*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrpzZETi4AyE"
      },
      "source": [
        "## Deep Learning 2\n",
        "\n",
        "### Exercise 2 -- Gradient Descent and momentum in numpy **(10 points)**\n",
        "\n",
        "While building a minimal  neural network (a linear model) in numpy, we define the parameters $\\theta$ as a matrix $W$ of shape `(8, 1)` and a bias vector $b$ of shape `(1,)`. You are given the functions to compute the loss and the gradient of the loss w.r.t. the parameters.\n",
        "\n",
        "1. write a function that performs gradient descent over `n_iter` steps with a given `learning rate`\n",
        "2. write a function that performs momentum gradient descent over `n_iter` steps with a given `learning rate` and `momentum`\n",
        "\n",
        "*Hint*: formula for the momentum update:\n",
        "\n",
        "$$ \\mathbf{v} \\leftarrow \\mu \\cdot \\mathbf{v} + \\nabla $$\n",
        "$$ \\mathbf{\\theta} \\leftarrow \\theta - \\eta \\cdot \\mathbf{v} $$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\theta$ is the vector of trainable parameters\n",
        "- $\\eta$ is the `learning_rate` coefficient\n",
        "- $\\mu$ is the `momentum` coefficient\n",
        "- $\\nabla$ is the gradient (usually of the loss function for the current value, in this case a random vector)\n",
        "- $\\mathbf{v}$ is the tensor of velocities and as the same shape as the parameters tensor $\\theta$. $\\mathbf{v}$ is initialized to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMkpAOFJ8uTG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n_samples = 100\n",
        "n_features = 8\n",
        "\n",
        "rng = np.random.RandomState(seed=0)\n",
        "X = rng.randn(n_samples, n_features)\n",
        "w_true = rng.randn(n_features)\n",
        "b_true = rng.randn(1)\n",
        "noise = rng.randn(n_samples) / 10\n",
        "y = X @ w_true + b_true + noise"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u99ktdI35WQ"
      },
      "source": [
        "def loss(params):\n",
        "    y_pred = X @ params[0] + params[1]\n",
        "    return np.mean(0.5 * (y - y_pred) ** 2, axis=0)\n",
        "\n",
        "\n",
        "def gradients(params):\n",
        "    y_pred = X @ params[0] + params[1]\n",
        "    diff = y_pred - y\n",
        "    return [np.mean(X * diff.reshape(-1, 1), axis=0),\n",
        "            np.mean(diff, axis=0)]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe729yMf4Zx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89bbfc7-8ee5-4aaa-cdd6-072a04392f14"
      },
      "source": [
        "init_params = [np.zeros(shape=(n_features,)),\n",
        "               np.zeros(shape=(1,))]\n",
        "\n",
        "loss(init_params)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.387651045016995"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWVylm9s4aj_"
      },
      "source": [
        "learning_rate = 0.1\n",
        "momentum = 0.5"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyRMWUza4dB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360acaa8-43a4-481f-ca8e-b298e54bff34"
      },
      "source": [
        "def gradient_descent(init_params, n_iter=5):\n",
        "    params = [p.copy() for p in init_params]\n",
        "    for step in range(n_iter):\n",
        "        new_gradients = gradients(params)\n",
        "        # write code to update the parameters with the\n",
        "        # gradients using gradient descent\n",
        "        params[0] -= learning_rate * new_gradients[0]\n",
        "        params[1] -= learning_rate * new_gradients[1]\n",
        "    return params\n",
        "\n",
        "\n",
        "final_params_gd = gradient_descent(init_params, n_iter=15)\n",
        "loss(final_params_gd)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10512056065005256"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOMuormO4eXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553b33ef-75ce-4d69-c349-c2041cb3aa0e"
      },
      "source": [
        "def momentum_gradient_descent(init_params, n_iter=5):\n",
        "    params = [p.copy() for p in init_params]\n",
        "    velocity = [np.zeros_like(p) for p in init_params]  # Initialize velocity\n",
        "    for step in range(n_iter):\n",
        "        new_gradients = gradients(params)\n",
        "        # write code to update the parameters with the\n",
        "        # gradients using momentum gradient descent\n",
        "           # Update velocity using momentum formula\n",
        "        velocity[0] = momentum * velocity[0] + learning_rate * new_gradients[0]\n",
        "        velocity[1] = momentum * velocity[1] + learning_rate * new_gradients[1]\n",
        "           # Update parameters using the updated velocity\n",
        "        params[0] -= velocity[0]\n",
        "        params[1] -= velocity[1]\n",
        "    return params\n",
        "\n",
        "final_params_mgd = momentum_gradient_descent(init_params, n_iter=15)\n",
        "loss(final_params_mgd)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.008094608613871715"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute loss for both methods\n",
        "loss_gd = loss(final_params_gd)\n",
        "loss_mgd = loss(final_params_mgd)\n",
        "\n",
        "print(\"Loss after Gradient Descent:\", round(loss_gd, 4))\n",
        "print(\"Loss after Momentum Gradient Descent:\", round(loss_mgd, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfNCnU_aGrA3",
        "outputId": "63647647-71e9-4422-a080-916412f90747"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after Gradient Descent: 0.1051\n",
            "Loss after Momentum Gradient Descent: 0.0081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN7euDyn4haG"
      },
      "source": [
        "### Exercise 3 -- Natural Language Classifier\n",
        "\n",
        "Alice wants to classify the topic of tweets. She is interested in knowing whether the tweet is dealing with `politics`, `technology`, `religion` or none of the 3. She supposes only one of these possibilities can happen for a given tweet.\n",
        "\n",
        "Say she has a dataset of 10K tweets with their corresponding label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTPUcqHy5Cb3"
      },
      "source": [
        "##### ðŸ”´ 3.3 Write the second and third models in the `elif` statements below **(10 points)**\n",
        "\n",
        "- model 2: based on LSTM with a sensible number of hidden units\n",
        "- model 3: based on several Convolutions1D and MaxPoolings with sensible numbers of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8vqFQvu4f6Z"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Input, Flatten, LSTM, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 5\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "model_num = 1\n",
        "N_CLASSES = 4\n",
        "\n",
        "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "if model_num == 1:\n",
        "    x = GlobalAveragePooling1D()(embedded_sequences)\n",
        "    predictions = Dense(N_CLASSES, activation='softmax')(x)\n",
        "elif model_num == 2:\n",
        "    # WRITE ME\n",
        "     # pass\n",
        "    x = LSTM(64)(embedded_sequences)  # LSTM layer with 64 hidden units\n",
        "    predictions = Dense(N_CLASSES, activation='softmax')(x)\n",
        "\n",
        "elif model_num == 3:\n",
        "    # WRITE ME\n",
        "    #  pass\n",
        "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)  # 128 filters with kernel size 5\n",
        "    x = MaxPooling1D(5)(x)  # MaxPooling with pool size 5\n",
        "    x = Conv1D(128, 5, activation='relu')(x)  # Another Conv1D layer\n",
        "    x = MaxPooling1D(5)(x)  # Another MaxPooling layer\n",
        "    x = Flatten()(x)\n",
        "    predictions = Dense(N_CLASSES, activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, predictions)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIWMd6mS5HJe"
      },
      "source": [
        "Use the following random input to check that you code can run without failing with randomly initialized weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA3Jejt95Hh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce35c8b-7b1a-4109-a17c-d0b2a5a8001a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_size = 3\n",
        "random_batch = np.random.randint(low=0, high=MAX_NB_WORDS,\n",
        "                                 size=(batch_size, MAX_SEQUENCE_LENGTH))\n",
        "model.predict(random_batch)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 331ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25016665, 0.25005895, 0.25002423, 0.24975015],\n",
              "       [0.24875106, 0.24847542, 0.25142512, 0.25134838],\n",
              "       [0.24903287, 0.24913466, 0.2506301 , 0.25120237]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WESmYUAIVoU",
        "outputId": "6d7525bc-e969-4e17-8fb0-51168b4778a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 50)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 50, 5)             100000    \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 5)                 0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 24        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 100024 (390.72 KB)\n",
            "Trainable params: 100024 (390.72 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su8VAyUG5VGA"
      },
      "source": [
        "Exercise 4 -- Captioning\n",
        "\n",
        "##### 4.1 Consider the following image-captioning model, which takes as input an image, and produces as output a sentence describing the image.\n",
        "\n",
        "<img src=\"https://i.imgur.com/QRS2Hrj.png\" style=\"width: 600px;\" />\n",
        "\n",
        "Notes:\n",
        "- All the images in the training set are 224x224 RGB color images.\n",
        "- Each sentence in the training set is an English sentence of maximum length 20, with words indexed as integers in a vocabulary of size 1000. There are special symbols; `<s>` for start of sequence and `<eos>` for end of sequence included in the 20 words length.\n",
        "- Henri does not one-hot encode the text part of the training data: he feeds the model directly with arrays of integer values as representation for the sequences.\n",
        "- During training, we use teacher forcing, which means we pass as input both the image and the shifted output text, and predict the next word.\n",
        "- The ResNet is pre-trained on ImageNet and outputs a vector representation of each image in dimension 2048, then a linear projection projects to a dimension of 128\n",
        "- For simplicity, we add the $h$ (`img_features` in the code below) image representation to the decoder's hidden activation $h_i^{dec}$ at each time step instead of just the first one, using `RepeatVector`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWpTCaHn5QQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867d479e-cf18-4095-bba5-2f96568f6d90"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten, SimpleRNN, RepeatVector, Lambda\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Embedding, Dot, Reshape, Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "base_model = ResNet50(include_top=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102967424/102967424 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPP27GpC5oh7"
      },
      "source": [
        "input_img = base_model.layers[0].input\n",
        "input_text = Input(shape=(20,), dtype='int32')\n",
        "\n",
        "MAX_NB_WORDS = 1000\n",
        "EMBEDDING_DIM = 128\n",
        "SEQ_LENGTH = 20\n",
        "\n",
        "# Image features: from the pre-trained resnet\n",
        "img_features = base_model.layers[-2].output\n",
        "img_features = Dense(EMBEDDING_DIM, use_bias=False)(img_features)\n",
        "img_features = RepeatVector(SEQ_LENGTH)(img_features)\n",
        "\n",
        "# Input text embedding\n",
        "input_text = Input(shape=(SEQ_LENGTH,), dtype='int32')\n",
        "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
        "                            input_length=SEQ_LENGTH)\n",
        "embedded_text = embedding_layer(input_text)\n",
        "\n",
        "# Combining the two and producing the output\n",
        "rnn_input = embedded_text + img_features\n",
        "output_seq = SimpleRNN(EMBEDDING_DIM)(rnn_input)\n",
        "output_seq = Dense(MAX_NB_WORDS, activation=\"softmax\")(output_seq)\n",
        "\n",
        "model = Model([input_img, input_text], output_seq)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y4i5X974yE_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}